{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data Loader.\n",
    "\n",
    "The functions in this script perform data load and a time series split.\n",
    "\n",
    "Usage:\n",
    "    Either run the whole pipeline (see src/main.py) or\n",
    "    import the functions.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_data(path_to_file: Path) -> pd.DataFrame:\n",
    "    \"\"\"Loads the main data file from csv to a pandas dataframe.\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    path_to_file : Path\n",
    "                Path to main csv.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "            Data as a dataframe.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path_to_file)\n",
    "\n",
    "    # convert time from string to datetime and set it as index\n",
    "    df.index = pd.to_datetime(df[\"time\"])\n",
    "    df = df.drop(columns=\"time\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def time_split(\n",
    "    df: pd.DataFrame, n_folds: int = 6, test_size: int = 9\n",
    ") -> List[Tuple[np.ndarray[int], np.ndarray[int]]]:\n",
    "    \"\"\"Creates an extending time series split for data.\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        Data as a dataframe.\n",
    "    n_folds : int, optional\n",
    "        Number of time series folds, default is 6.\n",
    "    test_size : int, optional\n",
    "        Number of rows in one test test, default is 9.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_splits : List[Tuple[np.ndarray[int], np.ndarray[int]]]\n",
    "                Splits of train and test indices per fold.\n",
    "    \"\"\"\n",
    "    all_splits = []\n",
    "    split_index = len(df) - n_folds * test_size\n",
    "    train_ids = np.arange(0, split_index)\n",
    "\n",
    "    for _ in range(1, n_folds + 1):\n",
    "        test_ids = np.arange(split_index, split_index + test_size)\n",
    "\n",
    "        all_splits.append((train_ids, test_ids))\n",
    "        train_ids = np.append(train_ids, test_ids)\n",
    "\n",
    "        split_index += test_size\n",
    "\n",
    "    return all_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = (\n",
    "    Path(\"..\")\n",
    "    / \"..\"\n",
    "    / \"hfactory_magic_folders\"\n",
    "    / \"plastic_cost_prediction\"\n",
    "    / \"data\"\n",
    ")\n",
    "MAIN_FILE = \"PA6_cleaned_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(DATA_DIR / MAIN_FILE)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_split(\n",
    "    df: pd.DataFrame, n_folds: int = 6, test_size: int = 9\n",
    ") -> List[Tuple[np.ndarray[int], np.ndarray[int]]]:\n",
    "    \"\"\"Creates an extending time series split for data.\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        Data as a dataframe.\n",
    "    n_folds : int, optional\n",
    "        Number of time series folds, default is 6.\n",
    "    test_size : int, optional\n",
    "        Number of rows in one test test, default is 9.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_splits : List[Tuple[np.ndarray[int], np.ndarray[int]]]\n",
    "                Splits of train and test indices per fold.\n",
    "    \"\"\"\n",
    "    all_splits = []\n",
    "    split_index = len(df) - n_folds * test_size\n",
    "    train_ids = np.arange(0, split_index)\n",
    "\n",
    "    for _ in range(1, n_folds + 1):\n",
    "        test_ids = np.arange(split_index, split_index + test_size)\n",
    "\n",
    "        all_splits.append((train_ids, test_ids))\n",
    "        train_ids = np.append(train_ids, test_ids)\n",
    "\n",
    "        split_index += test_size\n",
    "\n",
    "    return all_splits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check for stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adf_test(df):\n",
    "    non_stationary_columns = []\n",
    "\n",
    "    for column in df.columns:\n",
    "        print(f'\\nAugmented Dickey-Fuller Test for Column: {column}')\n",
    "        result = adfuller(df[column].dropna(), autolag='AIC')\n",
    "        labels = ['ADF test statistic', 'p-value', '# lags used', '# observations']\n",
    "        out = pd.Series(result[0:4], index=labels)\n",
    "        for key, val in result[4].items():\n",
    "            out[f'critical value ({key})'] = val\n",
    "        print(out.to_string())\n",
    "\n",
    "        if result[1] <= 0.05:\n",
    "            print(\"Strong evidence against the null hypothesis\")\n",
    "            print(\"Reject the null hypothesis\")\n",
    "            print(\"Data has no unit root and is stationary\")\n",
    "        else:\n",
    "            non_stationary_columns.append(column)\n",
    "            print(\"Weak evidence against the null hypothesis\")\n",
    "            print(\"Fail to reject the null hypothesis\")\n",
    "            print(\"Data has a unit root and is non-stationary\")\n",
    "\n",
    "    if not non_stationary_columns:\n",
    "        print(\"\\nAll columns are stationary.\")\n",
    "    else:\n",
    "        print(f\"\\nNon-stationary columns: {', '.join(non_stationary_columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1diff = df.diff()\n",
    "adf_test(df_1diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2diff = df_1diff.diff()\n",
    "adf_test(df_2diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3diff = df_2diff.diff()\n",
    "adf_test(df_3diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3diff.dropna(inplace=True)\n",
    "df_3diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl = time_split(df_3diff)\n",
    "\n",
    "for train_idx, test_idx in spl:\n",
    "    train = df_3diff.iloc[train_idx]\n",
    "    test = df_3diff.iloc[test_idx]\n",
    "\n",
    "for i in [1,2,3,4,5,6,7,8,9,10]:\n",
    "    model = VAR(train)\n",
    "    results = model.fit(i)\n",
    "    print('Order =', i)\n",
    "    print('AIC: ', results.aic)\n",
    "    print('BIC: ', results.bic)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit = model.fit(5)\n",
    "model_fit.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Durbin Watsonâ€™s Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.stattools import durbin_watson\n",
    "out = durbin_watson(model_fit.resid)\n",
    "\n",
    "for col, val in zip(df.columns, out):\n",
    "    print(col, ':', round(val, 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_order = model_fit.k_ar\n",
    "print(lag_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_input = df_difference_difference_difference.values[-lag_order:]\n",
    "forecast_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = model_fit.forecast(y=forecast_input, steps=nobs) # nobs defined at top of program\n",
    "df_forecast = pd.DataFrame(fc, index=df.index[-nobs:], columns=df.columns + '_3d')\n",
    "df_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_transformation(df_train, df_forecast):\n",
    "    df_fc = df_forecast.copy()\n",
    "    columns = df_train.columns\n",
    "\n",
    "    for col in columns:\n",
    "        # Roll back 3rd Diff\n",
    "        df_fc[str(col) + '_2d'] = df_train[col].iloc[-1] - df_train[col].iloc[-2] + df_fc[str(col) + '_3d'].cumsum()\n",
    "        df_fc[str(col) + '_1d'] = df_train[col].iloc[-2] - df_train[col].iloc[-3] + df_fc[str(col) + '_2d'].cumsum()\n",
    "        df_fc[str(col) + '_forecast'] = df_train[col].iloc[-3] + df_fc[str(col) + '_1d'].cumsum()\n",
    "\n",
    "    return df_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = invert_transformation(train, df_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=int(len(df.columns)/2), ncols=2, dpi=150, figsize=(20,20))\n",
    "for i, (col,ax) in enumerate(zip(df.columns, axes.flatten())):\n",
    "    df_results[col+'_forecast'].plot(legend=True, ax=ax).autoscale(axis='x',tight=True)\n",
    "    test[col][-nobs:].plot(legend=True, ax=ax);\n",
    "    ax.set_title(col + \": Forecast vs Actuals\")\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.spines[\"top\"].set_alpha(0)\n",
    "    ax.tick_params(labelsize=6)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_try_df = df.loc[:, test_df.columns != \"iNATGAS\"]\n",
    "first_try_df.dropna(inplace=True)\n",
    "first_try_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "def varmax_grid_search(train_df, order_range, seasonal_order_range):\n",
    "    \"\"\"\n",
    "    Perform a grid search to find the best parameters for VARMAX model based on AIC.\n",
    "\n",
    "    Parameters:\n",
    "    - train_df: pd.DataFrame, the training data\n",
    "    - order_range: tuple, range of values for the non-seasonal order parameter (p, q)\n",
    "    - seasonal_order_range: tuple, range of values for the seasonal order parameter (P, D, Q, s)\n",
    "\n",
    "    Returns:\n",
    "    - best_params: dict, the best parameters found during the grid search\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a grid of parameter combinations\n",
    "    param_grid = {\n",
    "        \"order\": [\n",
    "            (p, q)\n",
    "            for p in range(order_range[0], order_range[1] + 1)\n",
    "            for q in range(order_range[0], order_range[1] + 1)\n",
    "        ],\n",
    "        \"seasonal_order\": [\n",
    "            (P, D, Q, s)\n",
    "            for P in range(\n",
    "                seasonal_order_range[0], seasonal_order_range[1] + 1\n",
    "            )\n",
    "            for D in range(\n",
    "                seasonal_order_range[0], seasonal_order_range[1] + 1\n",
    "            )\n",
    "            for Q in range(\n",
    "                seasonal_order_range[0], seasonal_order_range[1] + 1\n",
    "            )\n",
    "            for s in range(1, 13)\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    best_aic = np.inf\n",
    "    best_params = None\n",
    "\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        # Create VARMAX model with current parameters\n",
    "        model = VARMAX(\n",
    "            train_df,\n",
    "            order=params[\"order\"],\n",
    "            seasonal_order=params[\"seasonal_order\"],\n",
    "            trend=\"c\",\n",
    "        )\n",
    "        try:\n",
    "            # Fit the model\n",
    "            model_fitted = model.fit(disp=False)\n",
    "\n",
    "            # Calculate AIC\n",
    "            current_aic = model_fitted.aic\n",
    "\n",
    "            # Update best parameters if the current AIC is lower\n",
    "            if current_aic < best_aic:\n",
    "                best_aic = current_aic\n",
    "                best_params = params\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting model with parameters {params}: {e}\")\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_range = (1, 3)\n",
    "seasonal_order_range = (1, 3)\n",
    "\n",
    "best_params = varmax_grid_search(\n",
    "    first_try_df, order_range, seasonal_order_range\n",
    ")\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
