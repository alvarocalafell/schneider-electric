{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = (\n",
    "    Path(\"..\")\n",
    "    / \"..\"\n",
    "    / \"..\"\n",
    "    / \"hfactory_magic_folders\"\n",
    "    / \"plastic_cost_prediction\"\n",
    "    / \"data\"\n",
    ")\n",
    "MAIN_FILE = \"PA6_cleaned_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_DIR / MAIN_FILE)\n",
    "# convert time from string to datetime\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouping_vars(df: pd.DataFrame) -> List[List[str]]:\n",
    "    \"\"\"Allows us to finds gorups of variables with the same prefix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        Dataframe for which we want to find groups.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    groups: Lsit[List[str]]\n",
    "        List with the groups of variables.\n",
    "    \"\"\"\n",
    "    prefix_dict = dict()\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    # finding the prefixes for each column and adding them to the dictionary.\n",
    "    for col in cols:\n",
    "        prefix = col.split(\"_\")[0]\n",
    "        if prefix in prefix_dict:\n",
    "            prefix_dict[prefix] += [col]\n",
    "        else:\n",
    "            prefix_dict[prefix] = [col]\n",
    "\n",
    "    groups = []\n",
    "\n",
    "    for _, value in prefix_dict.items():\n",
    "        if len(value) > 1:\n",
    "            groups += [value]\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = grouping_vars(df)\n",
    "CRUDE_vars = groups[0]\n",
    "NGAS_vars = groups[1]\n",
    "Electricity_vars = groups[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have acess to the time feature, we believe that creating a variable for year and month might help us in the future with our models. After knowing that the companies which produce PA6 have contracts for different energy ressources with a quarterly fixed rate, we have decided that from a business perspective, it made sense to add a variable representing the quarter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"year\"] = [df[\"time\"][i].year for i in range(len(df))]\n",
    "df[\"month\"] = [df[\"time\"][i].month for i in range(len(df))]\n",
    "df[\"quarter\"] = (df[\"month\"] - 1) // 3 + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our month and quarter variable are cyclical variables, we have decided to do a sine and cosine decomposition for both of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"month_sin\"] = np.sin(df[\"month\"] * 2 * np.pi / 12)\n",
    "df[\"month_cos\"] = np.cos(df[\"month\"] * 2 * np.pi / 12)\n",
    "df.drop(\"month\", axis=1, inplace=True)\n",
    "\n",
    "df[\"quarter_sin\"] = np.sin(df[\"quarter\"] * 2 * np.pi / 4)\n",
    "df[\"quarter_cos\"] = np.cos(df[\"quarter\"] * 2 * np.pi / 4)\n",
    "df.drop(\"quarter\", axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our initial dataset, we have 97 rows and 23 columns. Due to the curse of dimensionality, it is very important to try to diminish the number of features. With this in mind, we will now analyze the correlations between the features, in hopes of finding insights regarding which features can be removed or grouped together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_variables = [\n",
    "    \"time\",\n",
    "    \"year\",\n",
    "    \"month_sin\",\n",
    "    \"month_cos\",\n",
    "    \"quarter_sin\",\n",
    "    \"quarter_cos\",\n",
    "]\n",
    "data_for_corr = df.drop(time_variables, axis=1)\n",
    "plt.figure(figsize=(16, 6))\n",
    "heatmap = sns.heatmap(\n",
    "    data_for_corr.corr(), cmap=\"flare\", vmin=-1, vmax=1, annot=True\n",
    ")\n",
    "heatmap.set_title(\"Correlation Heatmap\", fontdict={\"fontsize\": 12}, pad=12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at this plot, and also by analyzing the features at our disposal, we can identify different groups: \n",
    "\n",
    "Crude Prices\n",
    "Natural Gas Prices\n",
    "Chemical prices\n",
    "Electricity Prices\n",
    "\n",
    "We will now investigate different ways to aggregate them, in order to find the most meaningful variables for our future models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crude Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CRUDE_AVG\"] = df[CRUDE_vars].mean(axis=1)\n",
    "df[\"CRUDE_MIN\"] = df[CRUDE_vars].min(axis=1)\n",
    "df[\"CRUDE_MAX\"] = df[CRUDE_vars].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crude = df[[\"CRUDE_AVG\", \"CRUDE_MIN\", \"CRUDE_MAX\", \"best_price_compound\"]]\n",
    "plt.figure(figsize=(10, 5))\n",
    "heatmap = sns.heatmap(\n",
    "    data_crude.corr(), cmap=\"flare\", vmin=-1, vmax=1, annot=True\n",
    ")\n",
    "heatmap.set_title(\"Correlation Heatmap\", fontdict={\"fontsize\": 12}, pad=12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all three  aggregated variables are very correlated with one another, and they all have around the same correlation with the target variable. Due to the mean being the more stable option, we decide to use the mean as the function we will use to aggregate the variables representing crude prices. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Gas Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_cols = [col for col in df.columns.to_list() if \"GAS\" in col]\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for column in gas_cols:\n",
    "    plt.plot(df[\"time\"], df[column], label=column)\n",
    "\n",
    "plt.title(f\"Time Series Plot - Natural Gas Prices\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"natural gas prices\")\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plot above, all the natural gas prices variables are very similar to one another except for iNATGAS. From the correlation matrix we can also see that iNATGAS has a correlation of 0.99 with NGAS_EUR. Knowing these two facts, and in order to avoid iNATGAS heavily influencing the aggregate variable we want to define, we will only aggregate the remaining natural gas variables, dropping the iNATGAS variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"NATGAS_AVG\"] = df[NGAS_vars].mean(axis=1)\n",
    "df[\"NATGAS_MIN\"] = df[NGAS_vars].min(axis=1)\n",
    "df[\"NATGAS_MAX\"] = df[NGAS_vars].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_NatGas = df[\n",
    "    [\"NATGAS_AVG\", \"NATGAS_MIN\", \"NATGAS_MAX\", \"best_price_compound\"]\n",
    "]\n",
    "plt.figure(figsize=(10, 5))\n",
    "heatmap = sns.heatmap(\n",
    "    data_NatGas.corr(), cmap=\"flare\", vmin=-1, vmax=1, annot=True\n",
    ")\n",
    "heatmap.set_title(\"Correlation Heatmap\", fontdict={\"fontsize\": 12}, pad=12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, NATGAS_AVG and NATGAS_MAX have a very high correlation, with NATGAS_MIN being less correlated to the other two aggregating variables. NATGAS_AVG is also the one with the biggest correlation to the target variable. Adding to this the fact that the mean is the most stable variable, we will use the mean to aggregate these three variables. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chemical Prices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three chemicals whose price is a feature of our dataset. They are Benzene, Cyclohexane and Caprolactam. Let us see how they are related to PA6 from a chemical point of view:\n",
    "\n",
    "Benzene --> Cyclohexane --> Caprolactam --> PA6\n",
    "\n",
    "As we can see, only Caprolactam is directly used in the production of PA6. After talking to an expert in Chemical Engineering, we obtained the information that the majority of companies which produce PA6 buy their Caprolactam instead of producing it. With this in mind, we have made the educated assumption that Caprolactam price will be much more influent in the prediction of the price of PA6 than the other variables and therefore we will discard the remaining chemical variables. \n",
    "\n",
    "We can also see that PA6 GLOBAL_ EMEAS _ EUR per TON has a correlation of 0.97 with Caprolactam_Price, and therefore we will drop it. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electricity Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Electricity_AVG\"] = df[Electricity_vars].mean(axis=1)\n",
    "df[\"Electricity_MIN\"] = df[Electricity_vars].min(axis=1)\n",
    "df[\"Electricity_MAX\"] = df[Electricity_vars].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_electricity = df[\n",
    "    [\n",
    "        \"Electricity_AVG\",\n",
    "        \"Electricity_MIN\",\n",
    "        \"Electricity_MAX\",\n",
    "        \"best_price_compound\",\n",
    "    ]\n",
    "]\n",
    "plt.figure(figsize=(10, 5))\n",
    "heatmap = sns.heatmap(\n",
    "    data_electricity.corr(), cmap=\"flare\", vmin=-1, vmax=1, annot=True\n",
    ")\n",
    "heatmap.set_title(\"Correlation Heatmap\", fontdict={\"fontsize\": 12}, pad=12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all three variables are very correlated and have a close to equal correlation to the target variable. Therefore, we have decided to use the mean to group the different variables regarding electricity prices. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation analysis of the processed variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now take a look at the correlation matrix for the variables after the preprocesing is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_for_corr = df[\n",
    "    [\n",
    "        \"Caprolactam_price\",\n",
    "        \"CRUDE_AVG\",\n",
    "        \"Electricity_AVG\",\n",
    "        \"NATGAS_AVG\",\n",
    "        \"Inflation_rate_france\",\n",
    "        \"Automotive Value\",\n",
    "        \"best_price_compound\",\n",
    "    ]\n",
    "]\n",
    "plt.figure(figsize=(16, 6))\n",
    "heatmap = sns.heatmap(\n",
    "    processed_data_for_corr.corr(), cmap=\"flare\", vmin=-1, vmax=1, annot=True\n",
    ")\n",
    "heatmap.set_title(\"Correlation Heatmap\", fontdict={\"fontsize\": 12}, pad=12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have managed to reduce drastically the ammount of variables which are extremely correlated with one another, as well as diminuishing the number of variables, one of our main goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schneider",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
