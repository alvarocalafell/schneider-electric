{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plastic Cost Prediction\n",
    "\n",
    "This project aims to develop a proof of concept for predicting plastic costs based on various factors using data analytics. The prediction will focus on understanding the correlation between plastic raw material prices and business trends. The project is being carried out by a team of students pursuing a Master of Science in Big Data for Business, in their second year.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "In today's business landscape, accurately predicting the costs associated with plastic materials is crucial for Schneider Electric (SE). However, it can be challenging to foresee how plastic costs will evolve in the future due to various factors influencing the market. To address this issue, leveraging data and AI technologies can provide valuable insights to forecast these costs effectively. By analyzing historical data, market trends, raw material prices, supply and demand dynamics, and economic indicators, we hope that we can develop a predictive model that helps businesses estimate future plastic costs with greater accuracy. This data-driven approach could empower Schneider Electric to make informed decisions, optimize its budgeting, and strategically plan its procurement strategies, ultimately maximizing profitability and minimizing financial risks associated with plastic materials.\n",
    "\n",
    "This exercise will try to tackle this issue by making a model to accurately predict the plastic raw material prices leveraging various data sources and AI.  \n",
    "\n",
    "### Expectations \n",
    "\n",
    "#### Main Expectations \n",
    "1. For Polyamide 6 (PA6) plastic raw material : we want to predict the price in 3, 6 & 9 months from now with Buying prices cost prediction value, trends and understand what contributed the most to the result (features importance)\n",
    " \n",
    "2. Looking in a second step at SE product selling prices and competitors selling prices histories from website distributors. How could you link Business trends and raw materials trends?\n",
    "Identify telling stories at Business level taking into account your raw material prediction.\n",
    "\n",
    "All those precious prediction would be used for Procurement negotiation, and/or Pricing strategy\n",
    "\n",
    "#### Data Science objectives\n",
    "We expect the students to take in consideration the following steps, this list is not exhaustive, other steps can be added. \n",
    "1. **State of the Art :** Research of scientific articles on raw material price / time-series forecasting\n",
    "2. **Data Preprocessing :** Apply the different data science techniques to sanitize the dataset and make it usable by AI models.\n",
    "3. **Feature Engineering :** Select the most relevant features, create new one...\n",
    "4. **Model Building :** Apply AI algorithms to train a predictive model and fine-tune the models. Students can use the libraries of their choice as long as they are open source and the licenses are verified. You are more than encouraged to test different models.  \n",
    "5. **Model Evaluation :** Assess the performance of the different models using appropriate evaluation metrics, including CO2 emissions.\n",
    "6. **Explainability :** Explain the results of the models and understand what impacted the most the results. \n",
    "7. **Ethical AI :** Being sure that the data is ethically sourced and that libraries are truly open-source.\n",
    "All those precious prediction would be used for Procurement negotiation, and/or Pricing strategy\n",
    "\n",
    "\n",
    "All those precious prediction would be used for Procurement negotiation, and/or Pricing strategy\n",
    "\n",
    "### Data Set\n",
    "\n",
    "**PA6_cleaned_dataset.csv**\n",
    "\n",
    "data source : concatenation of various sources<br>\n",
    "How : Public, private and intern data sources, monthly refresh<br>\n",
    "What : All tables of data have been selected and cleaned by type. Supplier Prices, Index prices, SE prices, PA6 substrat Prices, Energy prices, Automotive Market<br>\n",
    "``Comment : This is the main dataset that you will use for this challenge.``\n",
    "\n",
    "_Column explaination_ : <br>\n",
    "time : year-months-day<br>\n",
    "PA6 GLOBAL_ EMEAS _ EUR per TON : PA6 price for Europe in EUR/Ton, schneider index according to all PA6 product reference used in the company<br>\n",
    "CRUDE_PETRO,CRUDE_BRENT,CRUDE_DUBAI,CRUDE_WT : \"crude\" refers to the natural, unrefined state of the oil. It is the oil in its most basic form, before it has been processed or refined. Petro for canada, Brent for UK, Dubai for United Arab Emirates, WT for West Texas Intermediate (WTI) company.<br>\n",
    "NGAS_US,NGAS_EUR,NGAS_JP,iNATGAS : different types of natural gas from US, Europe, Japan and International Association for Natural Gas Vehicles. Gas/Energy is used a lot to transform oil and additives in plastic raw material. <br>\n",
    "best_price_compound : Our best SE buying price for PA6 compound in EUR/Kg in Europe, for confidentiality reason, these values have been modified but the trends are the same. <br>\n",
    "Benzene_price, Caprolactam_price, Cyclohexane_price : prices of the respective hydrocarbons in the market. Benzene is an aromatic hydrocarbon used in the production of various synthetic materials, while Caprolactam and Cyclohexane are cycloalkanes used in the production of nylon and other synthetic fibers<br> \n",
    "Electricty_Price_France,Electricty_Price_Italy,Electricty_Price_Poland,Electricty_Price_Netherlands,Electricty_Price_Germany : prices by country & months<br>\n",
    "Automotive Value : Automotive market (number of vehicules registred in France)\n",
    "\n",
    "**2023-10-16 history-export_GV2.xlsx**\n",
    "\n",
    "data source : Price Observatory <br>\n",
    "How : webscraping, dayly or weekly done from Partner website distributors<br>\n",
    "What : prices for GV2 Schneider Electric product in Europe (France, germany, Spain..), and all equivalent known product from Competition<br>\n",
    "What : date, SE price, all distributors prices, product URL, website, Designation, EAN, market place, seller<br>\n",
    "What for : Schneider Electric Pricing policy check<br>\n",
    "Comment : 2,2% of PA6 - 1,8% of PUR - 0,6% of PC - 13% of UP (polyester)<br>\n",
    "The main purpose of the TeSys GV2 thermal-magnetic motor circuit breaker is to protect three-phase motors, the cables, the people, against short circuits and overloads .\n",
    " \n",
    "**2023-10-16 history-export_IC60.xlsx**\n",
    "\n",
    "data source : Price Observatory <br>\n",
    "How : webscraping, dayly or weekly done from Partner website distributors<br>\n",
    "What : prices for IC60 Schneider Electric product in Europe (France, germany, Spain..), and all equivalent known product from Competition<br>\n",
    "What : date, SE price, all distributors prices, product URL, website, Designation, EAN, market place, seller<br>\n",
    "What for : Schneider Electric Pricing policy check<br>\n",
    "Comment : 33,2% of PA6 - 1,2% of PBT - 1,2% of PPS - 3,5% of PC <br>\n",
    "The main purpose of the iC60 circuit breaker is to ensure protection of low voltage electrical installations.\n",
    " \n",
    "**2023-10-16 history-export_Odace.xlsx**\n",
    "\n",
    "data source : Price Observatory <br>\n",
    "How : webscraping, dayly or weekly done from Partner website distributors<br>\n",
    "What : prices for IC60 Schneider Electric product in Europe (France, germany, Spain..), and all equivalent known product from Competition<br>\n",
    "What : date, SE price, all distributors prices, product URL, website, Designation, EAN, market place, seller<br>\n",
    "What for : Schneider Electric Pricing policy check<br>\n",
    "Comment : 20,14 of PA6 - 11% of PBT - 15% of ABS - 1% of PC <br>\n",
    "The main function of the ODACE Rotary 2 way switch dimmer 40-600 VA product range is to dim different light sources.\n",
    " \n",
    "**BASF.xlsx (balance Sheet)**\n",
    "\n",
    "History of BASF results.<br>\n",
    "datasource : Pitchbook software<br>\n",
    "What : Public dataset on quaterly basis<br>\n",
    "Excel file with all Financial data published by the company<br>\n",
    "What for : Analysis of Big player in Plastic raw material industry, that have a direct impact on market prices and trends\n",
    " \n",
    "**Commodity Price Watch Global tables_month.xlsx**\n",
    "\n",
    "History & prediction for raw material<br>\n",
    "data source : S&P Global Market intelligence<br>\n",
    "See introduction + Index worksheet (present in the Excel sheet)\n",
    " \n",
    "**WEOdateall_InflationGrowth.xlsx**\n",
    "\n",
    "IMF dataset on Inflation and Growth <br>\n",
    "See read me worksheet (present in the Excel sheet)\n",
    " \n",
    "**Statistic_id510959_global-number of-natural-disaster-event-2020-2022.xlsx**\n",
    "\n",
    "number of disaster counted by year<br>\n",
    "data source : Aon<br>\n",
    "See readme worksheet (present in the Excel sheet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to play ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages\n",
    "!pip install numpy==1.26.2\n",
    "!pip install pandas==2.1.3\n",
    "!pip install matplotlib==3.8.1\n",
    "!pip install statsmodels==0.14.0\n",
    "!pip install pmdarima==2.0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from pmdarima.arima import CHTest\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path_to_file: Path) -> pd.DataFrame:\n",
    "    \"\"\"Loads the main data file from csv to a pandas dataframe.\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    path_to_file : Path\n",
    "                Path to main csv.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : pd.DataFrame\n",
    "            Data as a dataframe.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(path_to_file)\n",
    "\n",
    "    # convert time from string to datetime and set it as index\n",
    "    data.index = pd.to_datetime(data[\"time\"])\n",
    "    data = data.drop(columns=\"time\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def time_split(\n",
    "    data: pd.DataFrame, n_folds: int = 6, test_size: int = 9\n",
    ") -> List[Tuple[np.ndarray[int], np.ndarray[int]]]:\n",
    "    \"\"\"Creates an extending time series split for data.\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    data : pd.DataFrame\n",
    "        Data as a dataframe.\n",
    "    n_folds : int, optional\n",
    "        Number of time series folds, default is 6.\n",
    "    test_size : int, optional\n",
    "        Number of rows in one test test, default is 9.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_splits : List[Tuple[np.ndarray[int], np.ndarray[int]]]\n",
    "                Splits of train and test indices per fold.\n",
    "    \"\"\"\n",
    "    all_splits = []\n",
    "    split_index = len(data) - n_folds * test_size\n",
    "    train_ids = np.arange(0, split_index)\n",
    "\n",
    "    for _ in range(1, n_folds + 1):\n",
    "        test_ids = np.arange(split_index, split_index + test_size)\n",
    "\n",
    "        all_splits.append((train_ids, test_ids))\n",
    "        train_ids = np.append(train_ids, test_ids)\n",
    "\n",
    "        split_index += test_size\n",
    "\n",
    "    return all_splits\n",
    "\n",
    "\n",
    "def smape(\n",
    "    y_true: Union[pd.Series, np.ndarray], y_pred: Union[pd.Series, np.ndarray]\n",
    ") -> float:\n",
    "    \"\"\"Calculates sMAPE between true and predicted values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : Union[pd.Series, np.ndarray]\n",
    "        Array of true values.\n",
    "    y_pred : Union[pd.Series, np.ndarray]\n",
    "        Array of predicted values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        SMAPE value, a percentage measure of the accuracy of the prediction.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        100\n",
    "        * np.sum(\n",
    "            2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))\n",
    "        )\n",
    "        / len(y_true)\n",
    "    )\n",
    "\n",
    "\n",
    "def mae(\n",
    "    y_true: Union[pd.Series, np.ndarray], y_pred: Union[pd.Series, np.ndarray]\n",
    ") -> float:\n",
    "    \"\"\"Calculates MAE between true and predicted values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : Union[pd.Series, np.ndarray]\n",
    "        Array of true values.\n",
    "    y_pred : Union[pd.Series, np.ndarray]\n",
    "        Array of predicted values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Mean Absolute Error between y_true and y_pred.\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = (\n",
    "    Path(\"..\")\n",
    "    / \"..\"\n",
    "    / \"hfactory_magic_folders\"\n",
    "    / \"plastic_cost_prediction\"\n",
    "    / \"data\"\n",
    ")\n",
    "MAIN_FILE = \"PA6_cleaned_dataset.csv\"\n",
    "OUTPUT_FILE = \"predictions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(DATA_DIR / MAIN_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using any type of models or machine learning technique, our team decided to first do Exploratory Data Analysis(EDA), in order to better understand the dataset. In order to find the various techniques used by the team while exploring the dataset, please check the dedicated notebook within the data_preprocessing folder. Within this notebook, and in order to be succinct, we have decided to only show the most important results, namely, the EDA results regarding our target variable, **best_price_compound**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.index, df[\"best_price_compound\"])\n",
    "plt.title(\"Time Series Plot - Best Price Compound\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Best Price Compound\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the Time Series Plot indicates that the data for our target variable is not stationary. Let us now do a Trend-Season-Residual decomposition as well as some seasonality tests in order to obtain more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplicative_decomposition = seasonal_decompose(\n",
    "    df[\"best_price_compound\"].dropna(), model=\"multiplicative\", period=30\n",
    ")\n",
    "\n",
    "plt.rcParams.update({\"figure.figsize\": (7, 5)})\n",
    "multiplicative_decomposition.plot().suptitle(\n",
    "    \"Multiplicative Decomposition\", fontsize=16\n",
    ")\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\"figure.figsize\": (8, 3), \"figure.dpi\": 120})\n",
    "autocorrelation_plot(df[\"best_price_compound\"].dropna());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHTest(m=2).estimate_seasonal_differencing_term(\n",
    "    df[\"best_price_compound\"].dropna()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our target variable follows a cubic trend without a clear seasonality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After observing the trend present in the target variable, and given that many of the models we wanted to try required stationarity, we decided to investigate possible stationarity transformations. Since for some of the models we wanted to test, we needed to be able to transform every variable into a stationary variable, there is a dedicated notebook within the data_preprocessing folder dedicated to this part of our project, which we recommend checking. However, within this notebook, we will only present what was done with regards to the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we did an Augmented Dickey-Fuller test in order to verify that our target variable was not stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(adfuller(df[\"best_price_compound\"].dropna())[1], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, since we have a p value of 0.3, our target variable is not stationarity. Therefore, we will try one of the most common methods in order to induce stationarity, the first difference, in order to try to obtain a stationary series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_target_variable = df[\"best_price_compound\"].diff().dropna()\n",
    "round(adfuller(transformed_target_variable)[1], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this transformation, we obtain a p value of 0.01 in the ADF test for the transformed target variable, and we can therefore conclude that we have been able to make our target variable stationary with the first difference transformation. Let us now look at the ACF and PACF plots for our transformed variable, in order to have some more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot original series\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(df[\"best_price_compound\"])\n",
    "plt.title(\"Original Series - Best Compound Price\")\n",
    "\n",
    "# Plot transformed series\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(transformed_target_variable)\n",
    "plt.title(\"Transformed Series - Best Compound Price\")\n",
    "\n",
    "# Plot ACF of transformed series\n",
    "plt.subplot(4, 1, 3)\n",
    "plot_acf(transformed_target_variable, lags=20, ax=plt.gca())\n",
    "plt.title(\"ACF - Best Compound Price\")\n",
    "\n",
    "# Plot PACF of transformed series\n",
    "plt.subplot(4, 1, 4)\n",
    "plot_pacf(transformed_target_variable, lags=20, method=\"ywm\", ax=plt.gca())\n",
    "plt.title(\"PACF - Best Compound Price\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing EDA and stationarity transformation, the team decided to try different models in order to try to obtain the most accurate and robust predictions for the price of PA6. Many different models were tested, such as:\n",
    "\n",
    "1. Univariate Models: Arima, ETS, XGboost\n",
    "2. Multivariate Models: Ensemble Models\n",
    "\n",
    "In order to be able to decide between different models, both for the same model class as well as comparing entirely different models, the metric used was the average sMAPE on a CV split with 6 folds, each of them having a test set with 9 data points, with us only taking into account the sMAPE for the 3rd,6th and 9th predictions per fold, in order to replicate the predictions for the 3rd, 6th, and 9th month that we must submit as a result. \n",
    "\n",
    "In order to better understand the full modelling process done by our team, please check the various notebooks present in the modeling folder. The best model that we found was a univariate ARIMA model for our target variable, with order (0,1,2). Let us now look at how our model performs for the different splits as well as the average sMAPE for the different months. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.DatetimeIndex(\n",
    "    [\n",
    "        \"2023-02-01\",\n",
    "        \"2023-03-01\",\n",
    "        \"2023-04-01\",\n",
    "        \"2023-05-01\",\n",
    "        \"2023-06-01\",\n",
    "        \"2023-07-01\",\n",
    "        \"2023-08-01\",\n",
    "        \"2023-09-01\",\n",
    "        \"2023-10-01\",\n",
    "    ]\n",
    ")\n",
    "# get time series for target and cv-splits as well as best model\n",
    "series = df[\"best_price_compound\"].copy().dropna()\n",
    "# target_model = ARIMA()\n",
    "spl = time_split(series)\n",
    "\n",
    "# create lists for target/preds for 3, 6, and 9 months for each fold\n",
    "preds_3, preds_6, preds_9 = [], [], []\n",
    "target_3, target_6, target_9 = [], [], []\n",
    "\n",
    "# create empty list for residuals\n",
    "res = []\n",
    "\n",
    "# create figure for residuals of each cv\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "plt.plot(series.index, series, label=\"Target\", color=\"blue\")\n",
    "plt.title(\"Target vs. Predictions per Fold\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Best Compound Price\")\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(spl):\n",
    "    # create train and test data and append target for fold fold\n",
    "    train = series.iloc[train_idx]\n",
    "    test = series.iloc[test_idx]\n",
    "    target_3.append(test.iloc[2])\n",
    "    target_6.append(test.iloc[5])\n",
    "    target_9.append(test.iloc[8])\n",
    "\n",
    "    # train model for fold\n",
    "    model = ARIMA(\n",
    "        train,\n",
    "        order=(0, 1, 2),\n",
    "        seasonal_order=(0, 0, 0, 0),\n",
    "    )\n",
    "    model_fit = model.fit()\n",
    "\n",
    "    # use model to predict on test index\n",
    "    # append predictions for 3, 6, and 9 months for fold\n",
    "    preds = model_fit.predict(start=test.index[0], end=test.index[-1])\n",
    "    preds_3.append(preds.iloc[2])\n",
    "    preds_6.append(preds.iloc[5])\n",
    "    preds_9.append(preds.iloc[8])\n",
    "\n",
    "    if i == 0:\n",
    "        plt.plot(test.index, preds, label=\"Validation\", color=\"orange\")\n",
    "    else:\n",
    "        plt.plot(test.index, preds, color=\"orange\")\n",
    "    plt.plot(test.index[2::3], preds[2::3], \".\", color=\"orange\")\n",
    "\n",
    "    # calculate residuals for all test data\n",
    "    res.append(test - preds)\n",
    "\n",
    "# train model on all data and add predictions to plot\n",
    "model = ARIMA(\n",
    "    series,\n",
    "    order=(0, 1, 2),\n",
    "    seasonal_order=(0, 0, 0, 0),\n",
    ")\n",
    "model_fit = model.fit()\n",
    "preds = model_fit.predict(start=idx[0], end=idx[-1])\n",
    "\n",
    "# plot predictions for future\n",
    "plt.plot(idx, preds, color=\"black\", label=\"Prediction\")\n",
    "plt.plot(idx[2::3], preds[2::3], \".\", color=\"black\")\n",
    "\n",
    "# add confidence bounds for future predictions\n",
    "conf = model_fit.get_forecast(steps=9).summary_frame()\n",
    "ax.fill_between(\n",
    "    conf.index,\n",
    "    conf[\"mean_ci_lower\"],\n",
    "    conf[\"mean_ci_upper\"],\n",
    "    color=\"k\",\n",
    "    alpha=0.1,\n",
    ")\n",
    "\n",
    "plt.legend(loc=\"upper left\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate sMAPE for 3, 6, and 9 months\n",
    "smape_3 = smape(np.array(target_3), np.array(preds_3))\n",
    "smape_6 = smape(np.array(target_6), np.array(preds_6))\n",
    "smape_9 = smape(np.array(target_9), np.array(preds_9))\n",
    "\n",
    "# calculate MAE for 3, 6, and 9 months\n",
    "mae_3 = mae(np.array(target_3), np.array(preds_3))\n",
    "mae_6 = mae(np.array(target_6), np.array(preds_6))\n",
    "mae_9 = mae(np.array(target_9), np.array(preds_9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"** Fold-averaged results for time horizons **\")\n",
    "print(\"3 months:\\n\", f\"- sMAPE: {smape_3:.2f}% \\n\", f\"- MAE: {mae_3:.2f}\")\n",
    "print(\"6 months:\\n\", f\"- sMAPE: {smape_6:.2f}% \\n\", f\"- MAE: {mae_6:.2f}\")\n",
    "print(\"9 months:\\n\", f\"- sMAPE: {smape_9:.2f}% \\n\", f\"- MAE: {mae_9:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the graph above, the predictions remain relatively stable for the different folds, which indicates that we have produced a solid and robust model that is able to generalize well. From the sMAPE values for 3,6 and 9 months we can see that as time goes on, the average sMAPE goes up, which indicates that our model becomes less powerful as time goes on. We can also see that after two steps our model converges. This is due to the lack of AR terms and the presence of only two MA terms, which lead to a convergence to the conditional mean after two steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residuals plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the predictions throughout  the folds, we also wanted to look at the residuals throughtout the folds, in order to understand if there was any trend shown, such as increased residuals throughout time, or an increase in variance. Let us look at the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residuals\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.title(\"Residuals\")\n",
    "plt.axhline(\n",
    "    0, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Zero Residuals\"\n",
    ")\n",
    "for residual in res:\n",
    "    plt.plot(residual.index, residual, label=\"Residuals\", color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the residuals do not show any trend or seasonality, they do not seem to increase, either in absolute terms or in variance as time goes on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have seen that the results of our baseline model(which is just taking the last value of the training set as the prediction, which is an ARIMA(0,1,0)),were not very different from our best model, we decided to look at the summary of our model, trained on the last fold, in order to understand the significance of our MA terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the results above, the two MA terms have a P value of close to 1, a lot bigger than the 0.05 value which would allow us to reject the null hypothesis that the terms are not significant. With this in mind, at least for the last fold, we can conclude that the two MA terms are not significant, which could mean that an ARIMA(0,1,0) model would be an equally viable solution, which would be more sustainable than this model, since it uses no machine learning. However, for different folds, these terms might be significant, and therefore, a more careful analysis would be required in order to make conclusion about the viability of the ARIMA(0,1,0) model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let us now show the prediction for April, July and October 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"** Predictions for April, July and October 2023: **\")\n",
    "for i, month in zip([2, 5, 8], [\"April\", \"July\", \"Octobre\"]):\n",
    "    print(\n",
    "        f\"{month} 2023:\\n\",\n",
    "        f\"- Predicted Best Compound Price: {preds.iloc[i]:.2f} \\n\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round final predictions to 2 decimal points\n",
    "final_predictions = round(preds.iloc[2::3], 2)\n",
    "\n",
    "# create dataframe with time and values\n",
    "final_predictions = pd.DataFrame(\n",
    "    {\n",
    "        \"time\": final_predictions.index,\n",
    "        \"best_price_compound\": final_predictions.values,\n",
    "    }\n",
    ")\n",
    "\n",
    "# save to csv\n",
    "final_predictions.to_csv(DATA_DIR / OUTPUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
